{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wgan import *\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "z_dim = 64\n",
    "display_step = 50  # Only for visualization of my output during training\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "c_lambda = 10\n",
    "\n",
    "crit_repeats = 5\n",
    "# Number of times the Critic will be trained for each Generator Training\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST(\n",
    "        \"/Users/abhinaybelde/Desktop/Learning/datasets/raw\", download=True, transform=transform\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "\n",
    "generator = Generator(z_dim).to(device) \n",
    "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "critic = Critic().to(device)\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "\n",
    "generator = generator.apply(weights_init)\n",
    "critic = critic.apply(weights_init)\n",
    "\n",
    "#######################################################################\n",
    "# Gradient Penalty Calculation -  Calculate Gradient of Critic Score\n",
    "#######################################################################\n",
    "def gradient_of_critic_score(critic, real, fake, epsilon):\n",
    "    \"\"\"\n",
    "    Function to compute the gradient of the critic's scores for interpolated images.\n",
    "\n",
    "    This function is a key component of the Gradient Penalty in WGAN-GP (Wasserstein GAN with Gradient Penalty),\n",
    "    a popular GAN architecture. The gradient penalty encourages the critic's gradient norms to be close to 1,\n",
    "    which ensures the 1-Lipschitz constraint needed for the Wasserstein distance function to be valid.\n",
    "\n",
    "    Args:\n",
    "        critic (nn.Module): The critic model, typically a neural network.\n",
    "        real (torch.Tensor): Batch of real images.\n",
    "        fake (torch.Tensor): Batch of fake images generated by the generator.\n",
    "        epsilon (float): The weight for the interpolation between real and fake images.\n",
    "\n",
    "    Returns:\n",
    "        gradient (torch.Tensor): The computed gradient of the critic's scores for the interpolated images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the interpolated images as a weighted combination of real and fake images\n",
    "    interpolated_images = real * epsilon + fake * (1 - epsilon)\n",
    "\n",
    "    mixed_scores = critic(interpolated_images)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    return gradient\n",
    "\n",
    "\"\"\" Whats the significane of the below line in above\n",
    "\n",
    "interpolated_images = real * epsilon + fake * (1 - epsilon)\n",
    "\n",
    " this line is used to enforce a constraint on the gradients of the critic (also called discriminator). This is known as the \"gradient penalty\".\n",
    "\n",
    "The idea behind the gradient penalty is to prevent the critic from becoming too powerful, which could cause the generator to fail to learn. This is done by encouraging the gradients of the critic to have a norm of 1 across the image space. To enforce this, we don't just consider the gradients at the real images or the fake images, but also at random points along the straight line between a pair of real and fake images. These points are the \"interpolated images\".\n",
    "\n",
    "By computing the gradients at these interpolated images and adding a penalty to the critic's loss function if these gradients deviate from 1, we can ensure that the critic is a 1-Lipschitz function, which is a key property needed for the theoretical guarantees of the Wasserstein distance to hold. This results in more stable training dynamics for the GAN.\n",
    "\n",
    "So, in essence, the line is generating the interpolated images at which the critic's gradients will be evaluated.\n",
    "\n",
    "\"\"\"\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Unit Test for above Method\n",
    "###############################################################################\n",
    "def test_gradient_of_critic_score(image_shape):\n",
    "    \"\"\"\n",
    "    Test the gradient_of_critic_score function by creating real and fake images and a random epsilon.\n",
    "\n",
    "    This function checks that the gradient has the correct shape and contains both positive and negative values.\n",
    "\n",
    "    Args:\n",
    "    image_shape (tuple): The shape of the images in the form of (batch_size, channels, height, width).\n",
    "\n",
    "    Returns:\n",
    "    gradient (tensor): The gradient calculated by the gradient_of_critic_score function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create real and fake images by adding and subtracting 1 to and from random numbers, respectively\n",
    "    real = torch.randn(*image_shape, device=device) + 1\n",
    "    fake = torch.randn(*image_shape, device=device) - 1\n",
    "\n",
    "    # Define the shape of epsilon, which should be the same as image_shape but with all dimensions except the first set to 1\n",
    "    epsilon_shape = [1 for _ in image_shape]  # [1, 1, 1, 1]\n",
    "    epsilon_shape[0] = image_shape[0]\n",
    "    # print(epsilon_shape)\n",
    "    # Create a random epsilon tensor that requires gradient\n",
    "    epsilon = torch.rand(epsilon_shape, device=device).requires_grad_()\n",
    "    # print(epsilon.shape)\n",
    "    # Compute the gradient of the critic score using the function gradient_of_critic_score\n",
    "    gradient = gradient_of_critic_score(critic, real, fake, epsilon)\n",
    "\n",
    "    # Check that the shape of the gradient matches image_shape\n",
    "    assert tuple(gradient.shape) == image_shape\n",
    "\n",
    "    # Check that the gradient contains both positive and negative values\n",
    "    assert gradient.max() > 0\n",
    "    assert gradient.min() < 0\n",
    "\n",
    "    # Return the gradient for potential further analysis or use\n",
    "    return gradient\n",
    "\n",
    "\n",
    "\n",
    "gradient = test_gradient_of_critic_score((256, 1, 28, 28))\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Gradient Penalty Calculation - Calculate the Penalty on The Norm of Gradient\n",
    "###############################################################################\n",
    "def gradient_penalty_l2_norm(gradient):\n",
    "    \"\"\"\n",
    "    Calculate the L2 norm of the gradient for enforcing the 1-Lipschitz constraint in Wasserstein GAN with Gradient Penalty (WGAN-GP).\n",
    "\n",
    "    The gradient penalty is calculated as the mean square error of the gradient norms from 1. The gradient penalty encourages the gradients of the critic to be unit norm, which is a key property of 1-Lipschitz functions.\n",
    "\n",
    "    Args:\n",
    "    gradient (torch.Tensor): The gradients of the critic's scores with respect to the interpolated images.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The gradient penalty.\n",
    "    \"\"\"\n",
    "    # Reshape each image in the batch into a 1D tensor (flatten the images)\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    # Calculate the penalty as the mean squared distance of the norms from 1.\n",
    "    penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "\n",
    "    return penalty\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "torch.sqrt(image_size)  tensor(28.)\n",
      "tensor(5.6843e-14)\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Unit Test for above Method\n",
    "###############################################################################\n",
    "def test_gradient_penalty_l2_norm(image_shape):\n",
    "    \"\"\"\n",
    "    Test the gradient_penalty_l2_norm function with different gradients.\n",
    "\n",
    "    This function creates gradients that are known to be bad, good, and random, and checks that the gradient penalty is high, low, and close to 1, respectively.\n",
    "\n",
    "    Args:\n",
    "    image_shape (tuple): The shape of the images in the form of (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    # Create a gradient of all zeros - this is a \"bad\" gradient because the norm is 0, not 1\n",
    "    bad_gradient = torch.zeros(*image_shape)\n",
    "\n",
    "\n",
    "    # Calculate the penalty for the bad gradient, should be high (1 in this case because (0-1)^2 = 1)\n",
    "    bad_gradient_penalty = gradient_penalty_l2_norm(bad_gradient)\n",
    "    print(bad_gradient_penalty)\n",
    "    assert torch.isclose(bad_gradient_penalty, torch.tensor(1.0))\n",
    "\n",
    "    # Calculate the size of an image in the batch\n",
    "    image_size = torch.prod(torch.Tensor(image_shape[1:]))  # 28 * 28 => 784\n",
    "\n",
    "    print(\"torch.sqrt(image_size) \", torch.sqrt(image_size))\n",
    "\n",
    "    # Create a gradient of all ones divided by the square root of the image size\n",
    "    # This is a \"good\" gradient because the norm is 1\n",
    "    good_gradient = torch.ones(*image_shape) / torch.sqrt(image_size)  # => tensor(28.)\n",
    "\n",
    "    # Calculate the penalty for the good gradient, should be low (0 in this case because (1-1)^2 = 0)\n",
    "    good_gradient_penalty = gradient_penalty_l2_norm(good_gradient)\n",
    "    print(good_gradient_penalty)\n",
    "    assert torch.isclose(good_gradient_penalty, torch.tensor(0.0))\n",
    "\n",
    "    # Create a random gradient by calling the gradient_of_critic_score function\n",
    "    random_gradient = test_gradient_of_critic_score(image_shape)\n",
    "\n",
    "    # Calculate the penalty for the random gradient, should be close to 1 if gradient_of_critic_score is working correctly\n",
    "    random_gradient_penalty = gradient_penalty_l2_norm(random_gradient)\n",
    "\n",
    "    assert torch.abs(random_gradient_penalty - 1) < 0.1\n",
    "\n",
    "\n",
    "\n",
    "test_gradient_penalty_l2_norm((256, 1, 28, 28))\n",
    "print(\"Success!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################\n",
    "# Final Training\n",
    "##############################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "current_step = 0\n",
    "generator_losses = []\n",
    "critic_losses_across_critic_repeats = []\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches\n",
    "    for real, _ in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        mean_critic_loss_for_this_iteration = 0\n",
    "        for _ in range(crit_repeats):\n",
    "\n",
    "            #########################\n",
    "            #  Train Critic\n",
    "            #########################\n",
    "            critic_optimizer.zero_grad()\n",
    "\n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "            fake = generator(fake_noise)\n",
    "\n",
    "            critic_fake_prediction = critic(fake.detach())\n",
    "\n",
    "            crit_real_pred = critic(real)\n",
    "\n",
    "            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
    "            # epsilon will be a Tensor of size torch.Size([128, 1, 1, 1]) for batch_size of 128\n",
    "\n",
    "            gradient = gradient_of_critic_score(critic, real, fake.detach(), epsilon)\n",
    "\n",
    "            gp = gradient_penalty_l2_norm(gradient)\n",
    "\n",
    "            crit_loss = get_crit_loss(\n",
    "                critic_fake_prediction, crit_real_pred, gp, c_lambda\n",
    "            )\n",
    "\n",
    "            # Keep track of the average critic loss in this batch\n",
    "            mean_critic_loss_for_this_iteration += crit_loss.item() / crit_repeats\n",
    "\n",
    "            # Update gradients\n",
    "            crit_loss.backward(retain_graph=True)\n",
    "            # Update optimizer i.e. the weights\n",
    "            critic_optimizer.step()\n",
    "        critic_losses_across_critic_repeats += [mean_critic_loss_for_this_iteration]\n",
    "\n",
    "        #########################\n",
    "        #  Train Generators\n",
    "        #########################\n",
    "        gen_optimizer.zero_grad()\n",
    "\n",
    "        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "        fake_2 = generator(fake_noise_2)\n",
    "\n",
    "        critic_fake_prediction = critic(fake_2)\n",
    "\n",
    "        gen_loss = get_gen_loss(critic_fake_prediction)\n",
    "\n",
    "        gen_loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [gen_loss.item()]\n",
    "\n",
    "        ##################################\n",
    "        #  Log Progress and Visualization\n",
    "        ##################################\n",
    "        # Do the below visualization for each display_step (i.e. each 50 step)\n",
    "        if current_step % display_step == 0 and current_step > 0:\n",
    "            # Calculate Generator Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            # list[-x:]   # last x items in the array\n",
    "            generator_mean_loss_display_step = (\n",
    "                sum(generator_losses[-display_step:]) / display_step\n",
    "            )\n",
    "\n",
    "            # Calculate Critic Mean loss for the latest display_steps (i.e. latest 50 steps)\n",
    "            critic_mean_loss_display_step = (\n",
    "                sum(critic_losses_across_critic_repeats[-display_step:]) / display_step\n",
    "            )\n",
    "            print(\n",
    "                f\"Step {current_step}: Generator loss: {generator_mean_loss_display_step}, critic loss: {critic_mean_loss_display_step}\"\n",
    "            )\n",
    "\n",
    "            # Plot both the real images and fake generated images\n",
    "            plot_images_from_tensor(fake)\n",
    "            plot_images_from_tensor(real)\n",
    "\n",
    "            step_bins = 20\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(generator_losses[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Generator Loss\",\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins),\n",
    "                torch.Tensor(critic_losses_across_critic_repeats[:num_examples])\n",
    "                .view(-1, step_bins)\n",
    "                .mean(1),\n",
    "                label=\"Critic Loss\",\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "        current_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
